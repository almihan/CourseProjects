{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Libraries Needed Through the Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "import wget\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definition of Import  File Function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(fid, fn):\n",
    "    durl = 'https://drive.google.com/' + 'uc?export=download&id='+ fid\n",
    "    print('downloading from', durl)\n",
    "    wget.download(durl, fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data From File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly chosen data:  ['uziah', 'nairobi', 'nayden', 'zana', 'ario']\n"
     ]
    }
   ],
   "source": [
    "file = 'yob2018.txt'\n",
    "with open(file,\"r\") as f:\n",
    "    train_names = [r.split(',')[0].lower().strip('\\n') for r in f]\n",
    "\n",
    "#ramdomly chose 5000 names from whole file.    \n",
    "data = random.choices(train_names, k=5000)\n",
    "print(\"Randomly chosen data: \",data[:5])\n",
    "\n",
    "#The characters are a-z (26 characters)  \n",
    "letters = string.ascii_lowercase\n",
    "n_letters = len(letters) + 1 # Plus EOS marker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting data to one_hot_encoded,  for input tensor, each batch contains 1x NÃ—M characters, where 1 is batch size,  N is the name length  and  M is the total letters with EOS marker  (#27). For input x, each name starts from first letter to last letter. For target, each input shifted by one which starts from second letter of input to EOS marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot matrix from first to last letters(not including the EOS) for input\n",
    "def inputTensor(name):\n",
    "    tensor = torch.zeros(1, len(name), n_letters)\n",
    "    for li in range(len(name)):\n",
    "        letter = name[li]\n",
    "        tensor[0][li][letters.find(letters)] = 1\n",
    "    return tensor\n",
    "\n",
    "# LongTensor from second letter to end(EOS) for target\n",
    "def targetTensor(name):\n",
    "    letter_indexes = [letters.find(name[li]) for li in range(1, len(name))]\n",
    "    letter_indexes.append(n_letters - 1) #EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharNet Model Definition (batch size is one, in each step one name is thrown in to the LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharNet(nn.Module):\n",
    "    def __init__(self, output_size,embedding_dim, hidden_dim, n_layers):\n",
    "        super(CharNet, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        #print(\"rr\",out.size())\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, 1, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, 1, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the  Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/200... Loss: 2.6911...\n",
      "Epoch: 50/200... Loss: 2.4826...\n",
      "Epoch: 100/200... Loss: 2.4436...\n",
      "Epoch: 150/200... Loss: 2.5146...\n",
      "Epoch: 200/200... Loss: 2.4607...\n"
     ]
    }
   ],
   "source": [
    "output_size = 27\n",
    "embedding_dim = 27\n",
    "hidden_dim = 128\n",
    "n_layers = 3\n",
    "\n",
    "model = CharNet(output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "lr=0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "epochs=200  \n",
    "batch_size=100    \n",
    "\n",
    "model.train()\n",
    "   \n",
    "for e in range(epochs+1):\n",
    "    train_loss = 0.0\n",
    "    n = len(data)\n",
    "    #randomly permutate the data array\n",
    "    idx = np.random.permutation(n)\n",
    "    for i in range(batch_size):\n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        inputs = inputTensor(data[idx[i]]) # tranform the name into one hot tensor\n",
    "        targets = targetTensor(data[idx[i]]) # obtain the targets\n",
    "\n",
    "        \n",
    "        h = model.init_hidden()\n",
    "        output, h = model(inputs, h)\n",
    "        output=output.squeeze()   \n",
    "        \n",
    "        loss = criterion(output,targets)\n",
    "            \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "    if (e%50==0): \n",
    "        print(\"Epoch: {}/{}...\".format(e, epochs),\n",
    "                      \"Loss: {:.4f}...\".format(train_loss/batch_size))\n",
    "        \n",
    "torch.save(model.state_dict(), 'CharNet.pth')       \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Google Drive Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading from https://drive.google.com/uc?export=download&id=1HpJ_k1qrcKLrvZx-uk2kLM0XfEo2KuUR\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://drive.google.com/open?id=1HpJ_k1qrcKLrvZx-uk2kLM0XfEo2KuUR\n",
    "fid = '1HpJ_k1qrcKLrvZx-uk2kLM0XfEo2KuUR'\n",
    "download(fid, 'CharNet.pth')\n",
    "\n",
    "output_size = 27\n",
    "embedding_dim = 27\n",
    "hidden_dim = 128\n",
    "n_layers = 3\n",
    "\n",
    "model = CharNet(output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "model.load_state_dict(torch.load(\"CharNet.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genarate name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aara\n",
      "Bnura\n",
      "Cor\n",
      "Mlin\n",
      "Nuyrctlana\n"
     ]
    }
   ],
   "source": [
    "# max lengh for generating name\n",
    "max_length = 10\n",
    "\n",
    "# sample from a starting letter\n",
    "def generate_name(net,start_letter):\n",
    "    global letters\n",
    "    global n_letters\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs = inputTensor(start_letter)\n",
    "        #print(inputs)\n",
    "        #inputs = inputs.to(device)\n",
    "        output_name = start_letter\n",
    "        #print(output_name)\n",
    "        \n",
    "        h = net.init_hidden()\n",
    "        for i in range(max_length):\n",
    "            #print(h)\n",
    "            output, h = net.forward(inputs, h)\n",
    "            #print(output)\n",
    "            #print(h)\n",
    "            p = F.softmax(output, dim=1).data\n",
    "            top_ch = np.arange(n_letters)\n",
    "            p = p.numpy().squeeze()\n",
    "                \n",
    "            index = np.random.choice(top_ch, p=p/p.sum())\n",
    "            if index == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter = letters[index]\n",
    "                output_name+=letter\n",
    "            inputs = inputTensor(letter)\n",
    "        \n",
    "        #capitalize starts letter of name   \n",
    "        print(output_name.capitalize())\n",
    "    \n",
    "\n",
    "for ch in 'abcmn':\n",
    "    generate_name(model,ch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating 5000 negetive samples randomly, then combineing them with randomly selected original 5000 names in task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('halee', 1.0), ('pierson', 1.0), ('dvmvl', 0.0), ('bee', 1.0), ('qjlcev', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "negative_samples=[]\n",
    "for i in range(5000):\n",
    "    st=''.join(random.choice(string.ascii_lowercase) for x in range(random.randint(3, 8)))\n",
    "    negative_samples.append(st)\n",
    "\n",
    "positive_samples=data\n",
    "\n",
    "\n",
    "#positive_samples=positive_samples.lower()\n",
    "positive_y = np.ones(len(positive_samples))\n",
    "negative_y = np.zeros(len(negative_samples))                   \n",
    "\n",
    "x = np.concatenate((positive_samples,negative_samples), axis=0) \n",
    "target= np.concatenate((positive_y,negative_y), axis=0) \n",
    "labeled_data= list(zip(x, target))\n",
    "np.random.shuffle(labeled_data)                      \n",
    "\n",
    "# final input and target data for task2\n",
    "x, y = zip(*labeled_data) \n",
    "print(labeled_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputTensor(name):\n",
    "    tensor = torch.zeros(1, len(name), n_letters)\n",
    "    for li in range(len(name)):\n",
    "        letter = name[li]\n",
    "        tensor[0][li][letters.find(letter)] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ClassNet Model Definition ( batch size is 1, each time one name is thrown into the LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassNet(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(ClassNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        out = out.view(1, -1)\n",
    "        \n",
    "        \n",
    "        #get last letter of name\n",
    "        out = out[:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, 1, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, 1, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize  the  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 1\n",
    "embedding_dim = 27\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "\n",
    "model = ClassNet(output_size, embedding_dim, hidden_dim, n_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100... Loss: 0.0981...\n",
      "Epoch: 50/100... Loss: 0.1253...\n",
      "Epoch: 100/100... Loss: 0.0416...\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "clip = 5\n",
    "batch_size=200\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.train()\n",
    "\n",
    "trainNum = len(x)\n",
    "\n",
    "\n",
    "for e in range(epochs+1):\n",
    "    \n",
    "    # randomly permutate the data array\n",
    "    idx = np.random.permutation(trainNum)\n",
    "    \n",
    "    train_loss=0.0\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        h = model.init_hidden()\n",
    "        #h = tuple([e.data for e in h])\n",
    "        model.zero_grad()\n",
    "        \n",
    "        inputs = inputTensor(x[idx[i]])\n",
    "        \n",
    "        #print(inputs.size())\n",
    "        \n",
    "        targets = y[idx[i]]\n",
    "       \n",
    "        outputs, h = model(inputs,h)\n",
    "        \n",
    "        targets=torch.tensor([targets])\n",
    "       \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "       \n",
    "    if (e%50==0): \n",
    "        print(\"Epoch: {}/{}...\".format(e, epochs),\n",
    "                      \"Loss: {:.4f}...\".format(train_loss/batch_size))\n",
    "torch.save(model.state_dict(), 'class.pth')         \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model google drive link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading from https://drive.google.com/uc?export=download&id=1YY_8mKnEhLF8HZRy-1a-LHeHtSyIST1E\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://drive.google.com/open?id=1YY_8mKnEhLF8HZRy-1a-LHeHtSyIST1E\n",
    "fid = '1YY_8mKnEhLF8HZRy-1a-LHeHtSyIST1E'\n",
    "download(fid, 'class.pth')\n",
    "\n",
    "output_size = 1\n",
    "embedding_dim = 27\n",
    "hidden_dim = 128\n",
    "n_layers = 1\n",
    "\n",
    "model = ClassNet(output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "model.load_state_dict(torch.load(\"class.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification for real or fake name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zion is Real!\n",
      "jaileigh is Real!\n",
      "lilyonna is Real!\n",
      "sgltryc is Fake!\n",
      "makaiya is Real!\n",
      "uwthio is Fake!\n",
      "azaryah is Real!\n",
      "timur is Fake!\n",
      "davia is Real!\n",
      "tyaira is Real!\n"
     ]
    }
   ],
   "source": [
    "def is_real_name(s):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        h = model.init_hidden()\n",
    "        h = tuple([each.data for each in h])    \n",
    "\n",
    "        model.eval()\n",
    "       \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        inputs = inputTensor(s)\n",
    "        \n",
    "        pred, h = model(inputs, h)\n",
    "               \n",
    "        if(pred>=threshhold):\n",
    "            print(\"%s is Real!\" % s)\n",
    "        else:\n",
    "            print(\"%s is Fake!\" % s)\n",
    "        \n",
    "        \n",
    "threshhold=0.5    \n",
    "\n",
    "#select 10 names from our x\n",
    "S=x[50:60]\n",
    "for s in S:\n",
    "    is_real_name(s)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
